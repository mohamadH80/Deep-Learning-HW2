{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNjZewcRNY9h/d0GutdR3bn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"W25VxYceHTlP","executionInfo":{"status":"error","timestamp":1699872206090,"user_tz":-210,"elapsed":7405,"user":{"displayName":"Mohamad.H Ostadi.V","userId":"09182486281119429867"}},"outputId":"63af31d0-5d9e-4812-a6bc-3e977a83eb07","colab":{"base_uri":"https://localhost:8080/","height":383}},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-6e0cf280af24>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'solver'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["\"\"\"\n","Implements fully connected networks in PyTorch.\n","WARNING: you SHOULD NOT use \".to()\" or \".cuda()\" in each implementation block.\n","\"\"\"\n","import torch\n","import solver\n","\n","\n","\n","class Linear(object):\n","\n","    @staticmethod\n","    def forward(x, w, b):\n","        \"\"\"\n","        Computes the forward pass for an linear (fully-connected) layer.\n","        The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n","        examples, where each example x[i] has shape (d_1, ..., d_k). We will\n","        reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n","        then transform it to an output vector of dimension M.\n","        Inputs:\n","        - x: A tensor containing input data, of shape (N, d_1, ..., d_k)\n","        - w: A tensor of weights, of shape (D, M)\n","        - b: A tensor of biases, of shape (M,)\n","        Returns a tuple of:\n","        - out: output, of shape (N, M)\n","        - cache: (x, w, b)\n","        \"\"\"\n","        out = None\n","        ######################################################################\n","        # TODO: Implement the linear forward pass. Store the result in out.  #\n","        # You will need to reshape the input into rows.                      #\n","        ######################################################################\n","        # Replace \"pass\" statement with your code\n","        # pass\n","        out = x.view(x.size(0), -1).mm(w) + b\n","        ######################################################################\n","        #                        END OF YOUR CODE                            #\n","        ######################################################################\n","\n","        cache = (x, w, b)\n","        return out, cache\n","\n","    @staticmethod\n","    def backward(dout, cache):\n","        \"\"\"\n","        Computes the backward pass for an linear layer.\n","        Inputs:\n","        - dout: Upstream derivative, of shape (N, M)\n","        - cache: Tuple of:\n","          - x: Input data, of shape (N, d_1, ... d_k)\n","          - w: Weights, of shape (D, M)\n","          - b: Biases, of shape (M,)\n","        Returns a tuple of:\n","        - dx: Gradient with respect to x, of shape\n","          (N, d1, ..., d_k)\n","        - dw: Gradient with respect to w, of shape (D, M)\n","        - db: Gradient with respect to b, of shape (M,)\n","        \"\"\"\n","        x, w, b = cache\n","        dx, dw, db = None, None, None\n","        ##################################################\n","        # TODO: Implement the linear backward pass.      #\n","        ##################################################\n","        # Replace \"pass\" statement with your code\n","        # pass\n","        dx = dout.mm(w.t()).view_as(x)\n","        dw = x.view(x.size(0), -1).t().mm(dout)\n","        db = dout.sum(dim=0)\n","        ##################################################\n","        #                END OF YOUR CODE                #\n","        ##################################################\n","        return dx, dw, db\n","\n","\n","class ReLU(object):\n","\n","    @staticmethod\n","    def forward(x):\n","        \"\"\"\n","        Computes the forward pass for a layer of rectified\n","        linear units (ReLUs).\n","        Input:\n","        - x: Input; a tensor of any shape\n","        Returns a tuple of:\n","        - out: Output, a tensor of the same shape as x\n","        - cache: x\n","        \"\"\"\n","        out = None\n","        ###################################################\n","        # TODO: Implement the ReLU forward pass.          #\n","        # You should not change the input tensor with an  #\n","        # in-place operation.                             #\n","        ###################################################\n","        # Replace \"pass\" statement with your code\n","        # pass\n","        out = x.clamp(min=0)\n","        ###################################################\n","        #                 END OF YOUR CODE                #\n","        ###################################################\n","        cache = x\n","        return out, cache\n","\n","    @staticmethod\n","    def backward(dout, cache):\n","        \"\"\"\n","        Computes the backward pass for a layer of rectified\n","        linear units (ReLUs).\n","        Input:\n","        - dout: Upstream derivatives, of any shape\n","        - cache: Input x, of same shape as dout\n","        Returns:\n","        - dx: Gradient with respect to x\n","        \"\"\"\n","        dx, x = None, cache\n","        #####################################################\n","        # TODO: Implement the ReLU backward pass.           #\n","        # You should not change the input tensor with an    #\n","        # in-place operation.                               #\n","        #####################################################\n","        # Replace \"pass\" statement with your code\n","        # pass\n","        x = cache\n","        dx = (x > 0) * dout\n","        #####################################################\n","        #                  END OF YOUR CODE                 #\n","        #####################################################\n","        return dx\n","\n","\n","class Linear_ReLU(object):\n","\n","    @staticmethod\n","    def forward(x, w, b):\n","        \"\"\"\n","        Convenience layer that performs an linear transform\n","        followed by a ReLU.\n","\n","        Inputs:\n","        - x: Input to the linear layer\n","        - w, b: Weights for the linear layer\n","        Returns a tuple of:\n","        - out: Output from the ReLU\n","        - cache: Object to give to the backward pass (hint: cache = (fc_cache, relu_cache))\n","        \"\"\"\n","        out = None\n","        cache = None\n","        ######################################################################\n","        # TODO: Implement the linear-relu forward pass.                      #\n","        ######################################################################\n","        # Replace \"pass\" statement with your code\n","        # pass\n","        linear_out, linear_cache = Linear.forward(x, w, b)\n","        relu_out, relu_cache = ReLU.forward(linear_out)\n","        out = relu_out\n","        cache = (linear_cache, relu_cache)\n","        ######################################################################\n","        #                        END OF YOUR CODE                            #\n","        ######################################################################\n","        return out, cache\n","\n","    @staticmethod\n","    def backward(dout, cache):\n","        \"\"\"\n","        Backward pass for the linear-relu convenience layer\n","        \"\"\"\n","        dx, dw, db = None, None, None\n","        ######################################################################\n","        # TODO: Implement the linear-relu backward pass.                     #\n","        ######################################################################\n","        # Replace \"pass\" statement with your code\n","        # pass\n","        linear_cache, relu_cache = cache\n","        drelu = ReLU.backward(dout, relu_cache)\n","        dx, dw, db = Linear.backward(drelu, linear_cache)\n","        ######################################################################\n","        #                END OF YOUR CODE                                    #\n","        ######################################################################\n","        return dx, dw, db\n","\n","\n","def softmax_loss(x, y):\n","    \"\"\"\n","    Computes the loss and gradient for softmax classification.\n","    Inputs:\n","    - x: Input data, of shape (N, C) where x[i, j] is the score for\n","      the jth class for the ith input.\n","    - y: Vector of labels, of shape (N,) where y[i] is the label\n","      for x[i] and 0 <= y[i] < C\n","    Returns a tuple of:\n","    - loss: Scalar giving the loss\n","    - dx: Gradient of the loss with respect to x\n","    \"\"\"\n","    loss = None\n","    dx = None\n","    ######################################################################\n","    # TODO: Implement the Softmax layer.                                 #\n","    ######################################################################\n","    # Replace \"pass\" statement with your code\n","    pass\n","    ######################################################################\n","    #                END OF YOUR CODE                                    #\n","    ######################################################################\n","    return loss, dx\n","\n","\n","class TwoLayerNet(object):\n","    \"\"\"\n","    A two-layer fully-connected neural network with ReLU nonlinearity and\n","    softmax loss that uses a modular layer design. We assume an input dimension\n","    of D, a hidden dimension of H, and perform classification over C classes.\n","    The architecure should be linear - relu - linear - softmax.\n","    Note that this class does not implement gradient descent; instead, it\n","    will interact with a separate Solver object that is responsible for running\n","    optimization.\n","\n","    The learnable parameters of the model are stored in the dictionary\n","    self.params that maps parameter names to PyTorch tensors.\n","    \"\"\"\n","\n","    def __init__(self, input_dim=3*32*32, hidden_dim=100, num_classes=10,\n","                 weight_scale=1e-3, reg=0.0,\n","                 dtype=torch.float32, device='cpu'):\n","        \"\"\"\n","        Initialize a new network.\n","        Inputs:\n","        - input_dim: An integer giving the size of the input\n","        - hidden_dim: An integer giving the size of the hidden layer\n","        - num_classes: An integer giving the number of classes to classify\n","        - weight_scale: Scalar giving the standard deviation for random\n","          initialization of the weights.\n","        - reg: Scalar giving L2 regularization strength.\n","        - dtype: A torch data type object; all computations will be\n","          performed using this datatype. float is faster but less accurate,\n","          so you should use double for numeric gradient checking.\n","        - device: device to use for computation. 'cpu' or 'cuda'\n","        \"\"\"\n","        self.params = {}\n","        self.reg = reg\n","\n","        ###################################################################\n","        # TODO: Initialize the weights and biases of the two-layer net.   #\n","        # Weights should be initialized from a Gaussian centered at       #\n","        # 0.0 with standard deviation equal to weight_scale, and biases   #\n","        # should be initialized to zero. All weights and biases should    #\n","        # be stored in the dictionary self.params, with first layer       #\n","        # weights and biases using the keys 'W1' and 'b1' and second layer#\n","        # weights and biases using the keys 'W2' and 'b2'.                #\n","        ###################################################################\n","        # Replace \"pass\" statement with your code\n","        # pass\n","        self.params = {\n","            'W1': weight_scale * torch.randn(input_dim, hidden_dim, dtype=dtype, device=device),\n","            'b1': torch.zeros(hidden_dim, dtype=dtype, device=device),\n","            'W2': weight_scale * torch.randn(hidden_dim, num_classes, dtype=dtype, device=device),\n","            'b2': torch.zeros(num_classes, dtype=dtype, device=device)\n","        }\n","        self.reg = reg\n","        ###############################################################\n","        #                            END OF YOUR CODE                 #\n","        ###############################################################\n","\n","    def save(self, path):\n","        checkpoint = {\n","          'reg': self.reg,\n","          'params': self.params,\n","        }\n","\n","        torch.save(checkpoint, path)\n","        print(\"Saved in {}\".format(path))\n","\n","    def load(self, path, dtype, device):\n","        checkpoint = torch.load(path, map_location='cpu')\n","        self.params = checkpoint['params']\n","        self.reg = checkpoint['reg']\n","        for p in self.params:\n","            self.params[p] = self.params[p].type(dtype).to(device)\n","        print(\"load checkpoint file: {}\".format(path))\n","\n","    def loss(self, X, y=None):\n","        \"\"\"\n","        Compute loss and gradient for a minibatch of data.\n","\n","        Inputs:\n","        - X: Tensor of input data of shape (N, d_1, ..., d_k)\n","        - y: int64 Tensor of labels, of shape (N,). y[i] gives the\n","          label for X[i].\n","\n","        Returns:\n","        If y is None, then run a test-time forward pass of the model\n","        and return:\n","        - scores: Tensor of shape (N, C) giving classification scores,\n","          where scores[i, c] is the classification score for X[i]\n","          and class c.\n","        If y is not None, then run a training-time forward and backward\n","        pass and return a tuple of:\n","        - loss: Scalar value giving the loss\n","        - grads: Dictionary with the same keys as self.params, mapping\n","          parameter names to gradients of the loss with respect to\n","          those parameters.\n","        \"\"\"\n","        scores = None\n","        #############################################################\n","        # TODO: Implement the forward pass for the two-layer net,   #\n","        # computing the class scores for X and storing them in the  #\n","        # scores variable.                                          #\n","        #############################################################\n","        # Replace \"pass\" statement with your code\n","        pass\n","        ##############################################################\n","        #                     END OF YOUR CODE                       #\n","        ##############################################################\n","\n","        # If y is None then we are in test mode so just return scores\n","        if y is None:\n","            return scores\n","\n","        loss, grads = 0, {}\n","        ###################################################################\n","        # TODO: Implement the backward pass for the two-layer net.        #\n","        # Store the loss in the loss variable and gradients in the grads  #\n","        # dictionary. Compute data loss using softmax, and make sure that #\n","        # grads[k] holds the gradients for self.params[k]. Don't forget   #\n","        # to add L2 regularization!                                       #\n","        #                                                                 #\n","        # NOTE: To ensure that your implementation matches ours and       #\n","        # you pass the automated tests, make sure that your L2            #\n","        # regularization does not include a factor of 0.5.                #\n","        ###################################################################\n","        # Replace \"pass\" statement with your code\n","        pass\n","        ###################################################################\n","        #                     END OF YOUR CODE                            #\n","        ###################################################################\n","\n","        return loss, grads\n","\n","\n","class FullyConnectedNet(object):\n","    \"\"\"\n","    A fully-connected neural network with an arbitrary number of hidden layers,\n","    ReLU nonlinearities, and a softmax loss function.\n","    For a network with L layers, the architecture will be:\n","\n","    {linear - relu - [dropout]} x (L - 1) - linear - softmax\n","\n","    where dropout is optional, and the {...} block is repeated L - 1 times.\n","\n","    Similar to the TwoLayerNet above, learnable parameters are stored in the\n","    self.params dictionary and will be learned using the Solver class.\n","    \"\"\"\n","\n","    def __init__(self, hidden_dims, input_dim=3*32*32, num_classes=10,\n","                 dropout=0.0, reg=0.0, weight_scale=1e-2, seed=None,\n","                 dtype=torch.float, device='cpu'):\n","        \"\"\"\n","        Initialize a new FullyConnectedNet.\n","\n","        Inputs:\n","        - hidden_dims: A list of integers giving the size of each\n","          hidden layer.\n","        - input_dim: An integer giving the size of the input.\n","        - num_classes: An integer giving the number of classes to classify.\n","        - dropout: Scalar between 0 and 1 giving the drop probability\n","          for networks with dropout. If dropout=0 then the network\n","          should not use dropout.\n","        - reg: Scalar giving L2 regularization strength.\n","        - weight_scale: Scalar giving the standard deviation for random\n","          initialization of the weights.\n","        - seed: If not None, then pass this random seed to the dropout\n","          layers. This will make the dropout layers deteriminstic so we\n","          can gradient check the model.\n","        - dtype: A torch data type object; all computations will be\n","          performed using this datatype. float is faster but less accurate,\n","          so you should use double for numeric gradient checking.\n","        - device: device to use for computation. 'cpu' or 'cuda'\n","        \"\"\"\n","        self.use_dropout = dropout != 0\n","        self.reg = reg\n","        self.num_layers = 1 + len(hidden_dims)\n","        self.dtype = dtype\n","        self.params = {}\n","\n","        #######################################################################\n","        # TODO: Initialize the parameters of the network, storing all         #\n","        # values in the self.params dictionary. Store weights and biases      #\n","        # for the first layer in W1 and b1; for the second layer use W2 and   #\n","        # b2, etc. Weights should be initialized from a normal distribution   #\n","        # centered at 0 with standard deviation equal to weight_scale. Biases #\n","        # should be initialized to zero.                                      #\n","        #######################################################################\n","        # Replace \"pass\" statement with your code\n","        # pass\n","        # Initialize parameters\n","        layer_input_dim = input_dim\n","        for i, h in enumerate(hidden_dims):\n","            self.params[f'W{i+1}'] = weight_scale * torch.randn(layer_input_dim, h, dtype=dtype, device=device)\n","            self.params[f'b{i+1}'] = torch.zeros(h, dtype=dtype, device=device)\n","            if self.use_dropout:\n","                self.params[f'dropout_param{i+1}'] = {'mode': 'train', 'p': dropout}\n","            layer_input_dim = h\n","\n","        self.params[f'W{self.num_layers}'] = weight_scale * torch.randn(layer_input_dim, num_classes, dtype=dtype, device=device)\n","        self.params[f'b{self.num_layers}'] = torch.zeros(num_classes, dtype=dtype, device=device)\n","        #######################################################################\n","        #                         END OF YOUR CODE                            #\n","        #######################################################################\n","\n","        # When using dropout we need to pass a dropout_param dictionary\n","        # to each dropout layer so that the layer knows the dropout\n","        # probability and the mode (train / test). You can pass the same\n","        # dropout_param to each dropout layer.\n","        self.dropout_param = {}\n","        if self.use_dropout:\n","            self.dropout_param = {'mode': 'train', 'p': dropout}\n","            if seed is not None:\n","                self.dropout_param['seed'] = seed\n","\n","    def save(self, path):\n","        checkpoint = {\n","          'reg': self.reg,\n","          'dtype': self.dtype,\n","          'params': self.params,\n","          'num_layers': self.num_layers,\n","          'use_dropout': self.use_dropout,\n","          'dropout_param': self.dropout_param,\n","        }\n","\n","        torch.save(checkpoint, path)\n","        print(\"Saved in {}\".format(path))\n","\n","    def load(self, path, dtype, device):\n","        checkpoint = torch.load(path, map_location='cpu')\n","        self.params = checkpoint['params']\n","        self.dtype = dtype\n","        self.reg = checkpoint['reg']\n","        self.num_layers = checkpoint['num_layers']\n","        self.use_dropout = checkpoint['use_dropout']\n","        self.dropout_param = checkpoint['dropout_param']\n","\n","        for p in self.params:\n","            self.params[p] = self.params[p].type(dtype).to(device)\n","\n","        print(\"load checkpoint file: {}\".format(path))\n","\n","    def loss(self, X, y=None):\n","        \"\"\"\n","        Compute loss and gradient for the fully-connected net.\n","        Input / output: Same as TwoLayerNet above.\n","        \"\"\"\n","        X = X.to(self.dtype)\n","        mode = 'test' if y is None else 'train'\n","\n","        # Set train/test mode for batchnorm params and dropout param\n","        # since they behave differently during training and testing.\n","        if self.use_dropout:\n","            self.dropout_param['mode'] = mode\n","        scores = None\n","        ##################################################################\n","        # TODO: Implement the forward pass for the fully-connected net,  #\n","        # computing the class scores for X and storing them in the       #\n","        # scores variable.                                               #\n","        #                                                                #\n","        # When using dropout, you'll need to pass self.dropout_param     #\n","        # to each dropout forward pass.                                  #\n","        ##################################################################\n","        # Replace \"pass\" statement with your code\n","        pass\n","        #################################################################\n","        #                      END OF YOUR CODE                         #\n","        #################################################################\n","\n","        # If test mode return early\n","        if mode == 'test':\n","            return scores\n","\n","        loss, grads = 0.0, {}\n","        #####################################################################\n","        # TODO: Implement the backward pass for the fully-connected net.    #\n","        # Store the loss in the loss variable and gradients in the grads    #\n","        # dictionary. Compute data loss using softmax, and make sure that   #\n","        # grads[k] holds the gradients for self.params[k]. Don't forget to  #\n","        # add L2 regularization!                                            #\n","        # NOTE: To ensure that your implementation matches ours and you     #\n","        # pass the automated tests, make sure that your L2 regularization   #\n","        # includes a factor of 0.5 to simplify the expression for           #\n","        # the gradient.                                                     #\n","        #####################################################################\n","        # Replace \"pass\" statement with your code\n","        pass\n","        ###########################################################\n","        #                   END OF YOUR CODE                      #\n","        ###########################################################\n","\n","        return loss, grads\n","\n","\n","def create_solver_instance(data_dict, dtype, device):\n","    model = TwoLayerNet(hidden_dim=200, dtype=dtype, device=device)\n","    #############################################################\n","    # TODO: Use a Solver instance to train a TwoLayerNet that   #\n","    # achieves at least 50% accuracy on the validation set.     #\n","    #############################################################\n","    solver = None\n","    # Replace \"pass\" statement with your code\n","    pass\n","    ##############################################################\n","    #                    END OF YOUR CODE                        #\n","    ##############################################################\n","    return solver\n","\n","\n","def get_three_layer_network_params():\n","    ###############################################################\n","    # TODO: Change weight_scale and learning_rate so your         #\n","    # model achieves 100% training accuracy within 20 epochs.     #\n","    ###############################################################\n","    weight_scale = 1e-2   # Experiment with this!\n","    learning_rate = 1e-4  # Experiment with this!\n","    ################################################################\n","    #                             END OF YOUR CODE                 #\n","    ################################################################\n","    return weight_scale, learning_rate\n","\n","\n","def get_five_layer_network_params():\n","    ################################################################\n","    # TODO: Change weight_scale and learning_rate so your          #\n","    # model achieves 100% training accuracy within 20 epochs.      #\n","    ################################################################\n","    learning_rate = 2e-3  # Experiment with this!\n","    weight_scale = 1e-5   # Experiment with this!\n","    ################################################################\n","    #                       END OF YOUR CODE                       #\n","    ################################################################\n","    return weight_scale, learning_rate\n","\n","\n","def sgd(w, dw, config=None):\n","    \"\"\"\n","    Performs vanilla stochastic gradient descent.\n","    config format:\n","    - learning_rate: Scalar learning rate.\n","    \"\"\"\n","    if config is None:\n","        config = {}\n","    config.setdefault('learning_rate', 1e-2)\n","\n","    w -= config['learning_rate'] * dw\n","    return w, config\n","\n","\n","def sgd_momentum(w, dw, config=None):\n","    \"\"\"\n","    Performs stochastic gradient descent with momentum.\n","    config format:\n","    - learning_rate: Scalar learning rate.\n","    - momentum: Scalar between 0 and 1 giving the momentum value.\n","      Setting momentum = 0 reduces to sgd.\n","    - velocity: A numpy array of the same shape as w and dw used to\n","      store a moving average of the gradients.\n","    \"\"\"\n","    if config is None:\n","        config = {}\n","    config.setdefault('learning_rate', 1e-2)\n","    config.setdefault('momentum', 0.9)\n","    v = config.get('velocity', torch.zeros_like(w))\n","\n","    next_w = None\n","    ##################################################################\n","    # TODO: Implement the momentum update formula. Store the         #\n","    # updated value in the next_w variable. You should also use and  #\n","    # update the velocity v.                                         #\n","    ##################################################################\n","    # Replace \"pass\" statement with your code\n","    pass\n","    ###################################################################\n","    #                           END OF YOUR CODE                      #\n","    ###################################################################\n","    config['velocity'] = v\n","\n","    return next_w, config\n","\n","\n","def rmsprop(w, dw, config=None):\n","    \"\"\"\n","    Uses the RMSProp update rule, which uses a moving average of squared\n","    gradient values to set adaptive per-parameter learning rates.\n","    config format:\n","    - learning_rate: Scalar learning rate.\n","    - decay_rate: Scalar between 0 and 1 giving the decay rate for the squared\n","      gradient cache.\n","    - epsilon: Small scalar used for smoothing to avoid dividing by zero.\n","    - cache: Moving average of second moments of gradients.\n","    \"\"\"\n","    if config is None:\n","        config = {}\n","    config.setdefault('learning_rate', 1e-2)\n","    config.setdefault('decay_rate', 0.99)\n","    config.setdefault('epsilon', 1e-8)\n","    config.setdefault('cache', torch.zeros_like(w))\n","\n","    next_w = None\n","    ###########################################################################\n","    # TODO: Implement the RMSprop update formula, storing the next value of w #\n","    # in the next_w variable. Don't forget to update cache value stored in    #\n","    # config['cache'].                                                        #\n","    ###########################################################################\n","    # Replace \"pass\" statement with your code\n","    pass\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","\n","    return next_w, config\n","\n","\n","def adam(w, dw, config=None):\n","    \"\"\"\n","    Uses the Adam update rule, which incorporates moving averages of both the\n","    gradient and its square and a bias correction term.\n","    config format:\n","    - learning_rate: Scalar learning rate.\n","    - beta1: Decay rate for moving average of first moment of gradient.\n","    - beta2: Decay rate for moving average of second moment of gradient.\n","    - epsilon: Small scalar used for smoothing to avoid dividing by zero.\n","    - m: Moving average of gradient.\n","    - v: Moving average of squared gradient.\n","    - t: Iteration number.\n","    \"\"\"\n","    if config is None:\n","        config = {}\n","    config.setdefault('learning_rate', 1e-3)\n","    config.setdefault('beta1', 0.9)\n","    config.setdefault('beta2', 0.999)\n","    config.setdefault('epsilon', 1e-8)\n","    config.setdefault('m', torch.zeros_like(w))\n","    config.setdefault('v', torch.zeros_like(w))\n","    config.setdefault('t', 0)\n","\n","    next_w = None\n","    ##########################################################################\n","    # TODO: Implement the Adam update formula, storing the next value of w in#\n","    # the next_w variable. Don't forget to update the m, v, and t variables  #\n","    # stored in config.                                                      #\n","    #                                                                        #\n","    # NOTE: In order to match the reference output, please modify t _before_ #\n","    # using it in any calculations.                                          #\n","    ##########################################################################\n","    # Replace \"pass\" statement with your code\n","    pass\n","    #########################################################################\n","    #                              END OF YOUR CODE                         #\n","    #########################################################################\n","\n","    return next_w, config\n","\n","\n","class Dropout(object):\n","\n","    @staticmethod\n","    def forward(x, dropout_param):\n","        \"\"\"\n","        Performs the forward pass for (inverted) dropout.\n","        Inputs:\n","        - x: Input data: tensor of any shape\n","        - dropout_param: A dictionary with the following keys:\n","          - p: Dropout parameter. We *drop* each neuron output with\n","            probability p.\n","          - mode: 'test' or 'train'. If the mode is train, then\n","            perform dropout;\n","          if the mode is test, then just return the input.\n","          - seed: Seed for the random number generator. Passing seed\n","            makes this\n","            function deterministic, which is needed for gradient checking\n","            but not in real networks.\n","        Outputs:\n","        - out: Tensor of the same shape as x.\n","        - cache: tuple (dropout_param, mask). In training mode, mask\n","          is the dropout mask that was used to multiply the input; in\n","          test mode, mask is None.\n","        NOTE: Please implement **inverted** dropout, not the vanilla\n","              version of dropout.\n","        See http://cs231n.github.io/neural-networks-2/#reg for more details.\n","        NOTE 2: Keep in mind that p is the probability of **dropping**\n","                a neuron output; this might be contrary to some sources,\n","                where it is referred to as the probability of keeping a\n","                neuron output.\n","        \"\"\"\n","        p, mode = dropout_param['p'], dropout_param['mode']\n","        if 'seed' in dropout_param:\n","            torch.manual_seed(dropout_param['seed'])\n","\n","        mask = None\n","        out = None\n","\n","        if mode == 'train':\n","            ##############################################################\n","            # TODO: Implement training phase forward pass for            #\n","            # inverted dropout.                                          #\n","            # Store the dropout mask in the mask variable.               #\n","            ##############################################################\n","            # Replace \"pass\" statement with your code\n","            pass\n","            ##############################################################\n","            #                   END OF YOUR CODE                         #\n","            ##############################################################\n","        elif mode == 'test':\n","            ##############################################################\n","            # TODO: Implement the test phase forward pass for            #\n","            # inverted dropout.                                          #\n","            ##############################################################\n","            # Replace \"pass\" statement with your code\n","            pass\n","            ##############################################################\n","            #                      END OF YOUR CODE                      #\n","            ##############################################################\n","\n","        cache = (dropout_param, mask)\n","\n","        return out, cache\n","\n","    @staticmethod\n","    def backward(dout, cache):\n","        \"\"\"\n","        Perform the backward pass for (inverted) dropout.\n","        Inputs:\n","        - dout: Upstream derivatives, of any shape\n","        - cache: (dropout_param, mask) from Dropout.forward.\n","        \"\"\"\n","        dropout_param, mask = cache\n","        mode = dropout_param['mode']\n","\n","        dx = None\n","        if mode == 'train':\n","            ###########################################################\n","            # TODO: Implement training phase backward pass for        #\n","            # inverted dropout                                        #\n","            ###########################################################\n","            # Replace \"pass\" statement with your code\n","            pass\n","            ###########################################################\n","            #                     END OF YOUR CODE                    #\n","            ###########################################################\n","        elif mode == 'test':\n","            dx = dout\n","        return dx\n","\n"]}]}